{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the goal is to develop a classification model to help Megaline, a mobile carrier, recommend suitable new plans (Smart or Ultra) to subscribers still using legacy plans. By analyzing subscriber behavior data, the model aims to achieve an accuracy of at least 75%, improving Megalineâ€™s ability to target customers with the most appropriate plan. The project focuses on building the model after the data preprocessing stage has already been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>58.0</td>\n",
       "      <td>344.56</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15823.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57.0</td>\n",
       "      <td>431.64</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3738.90</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.0</td>\n",
       "      <td>132.40</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21911.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>43.39</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2538.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>90.0</td>\n",
       "      <td>665.41</td>\n",
       "      <td>38.0</td>\n",
       "      <td>17358.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  minutes  messages   mb_used  is_ultra\n",
       "0   40.0   311.90      83.0  19915.42         0\n",
       "1   85.0   516.75      56.0  22696.96         0\n",
       "2   77.0   467.66      86.0  21060.45         0\n",
       "3  106.0   745.53      81.0   8437.39         1\n",
       "4   66.0   418.74       1.0  14502.75         0\n",
       "5   58.0   344.56      21.0  15823.37         0\n",
       "6   57.0   431.64      20.0   3738.90         1\n",
       "7   15.0   132.40       6.0  21911.60         0\n",
       "8    7.0    43.39       3.0   2538.67         1\n",
       "9   90.0   665.41      38.0  17358.61         0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import data file\n",
    "behaviors_df = pd.read_csv('users_behavior.csv')\n",
    "behaviors_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data into Training set, validation set, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2571, 4)\n",
      "(2571,)\n",
      "(643, 4)\n",
      "(643,)\n",
      "(804, 4)\n",
      "(804,)\n"
     ]
    }
   ],
   "source": [
    "# run train_test_split function\n",
    "behaviors_df_train, behaviors_df_valid = train_test_split(behaviors_df, test_size=0.25, random_state=12345)\n",
    "behaviors_df_train, behaviors_df_test = train_test_split(behaviors_df, test_size=0.20, random_state=12345)\n",
    "\n",
    "# Training set\n",
    "features_train = behaviors_df_train.drop(['is_ultra'], axis=1)\n",
    "target_train = behaviors_df_train['is_ultra']\n",
    "\n",
    "# test set\n",
    "test_features = behaviors_df_test.drop(['is_ultra'], axis=1) \n",
    "test_target = behaviors_df_test['is_ultra']\n",
    "\n",
    "# Validation set\n",
    "features_valid = behaviors_df_valid.drop(['is_ultra'], axis=1)\n",
    "target_valid = behaviors_df_valid['is_ultra']\n",
    "\n",
    "print(features_train.shape)\n",
    "print(target_train.shape)\n",
    "print(test_features.shape)\n",
    "print(test_target.shape)\n",
    "print(features_valid.shape)\n",
    "print(target_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing quality of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 1 :  0.75\n",
      "max_depth = 2 :  0.7835820895522388\n",
      "max_depth = 3 :  0.7885572139303483\n",
      "max_depth = 4 :  0.7848258706467661\n",
      "max_depth = 5 :  0.7898009950248757\n",
      "max_depth = 6 :  0.7898009950248757\n",
      "max_depth = 7 :  0.7885572139303483\n",
      "max_depth = 8 :  0.7810945273631841\n",
      "max_depth = 9 :  0.7910447761194029\n",
      "max_depth = 10 :  0.7860696517412935\n"
     ]
    }
   ],
   "source": [
    "# DecisionTree model\n",
    "for depth in range(1,11):\n",
    "    decision_tree_model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    decision_tree_model.fit(features_train, target_train)\n",
    "    prediction_valid = decision_tree_model.predict(features_valid)\n",
    "    print('max_depth =', depth, ': ', end=' ')\n",
    "    print(accuracy_score(target_valid, prediction_valid)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth = 1 :  0.7222222222222222\n",
      "max_depth = 2 :  0.7637795275590551\n",
      "max_depth = 3 :  0.7591240875912408\n",
      "max_depth = 4 :  0.753731343283582\n",
      "max_depth = 5 :  0.7608695652173914\n",
      "max_depth = 6 :  0.7432432432432432\n",
      "max_depth = 7 :  0.7983193277310925\n",
      "max_depth = 8 :  0.7210884353741497\n",
      "max_depth = 9 :  0.7703703703703704\n",
      "max_depth = 10 :  0.7346938775510204\n"
     ]
    }
   ],
   "source": [
    "#testing the precision score funtion\n",
    "for depth in range(1,11):\n",
    "    model = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model.fit(features_train, target_train)\n",
    "    prediction_valid = model.predict(features_valid)\n",
    "    print('max_depth =', depth, ': ', end=' ')\n",
    "    print(precision_score(target_valid, prediction_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max Depth 9 is performing above the accuracy threshold of 0.75, suggesting they have a good fit for the data, at least based on the validation set. However this could be due to overfitting. With that in mind I decided to also run the data with the precision score function to see a different view of the model's performance. Based on the precision score max depth 7 may be slightly better as the score drops when it gets to max depth 8 signaling it could be the start of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best model accuracy on validation set (n_estimators = 10): 0.8208955223880597'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RandomForestClassifier\n",
    "best_score = 0\n",
    "best_est = 0\n",
    "for est in range(1, 11):\n",
    "    random_forest_model = RandomForestClassifier(random_state=54321, n_estimators=est) # max_depth=10, min_samples_split=5, min_samples_leaf=5)\n",
    "    random_forest_model.fit(features_train, target_train)\n",
    "    score = random_forest_model.score(features_valid, target_valid)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_est = est\n",
    "display(\"Best model accuracy on validation set (n_estimators = {}): {}\".format(best_est, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best model accuracy on training set (n_estimators = 10):'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RandomForestClassifier - Final Model\n",
    "final_model = RandomForestClassifier(random_state=54321, n_estimators=10) #best estimator was 10 from previous code\n",
    "final_model.fit(features_train, target_train)\n",
    "display(\"Best model accuracy on training set (n_estimators = {}):\".format(best_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the code through a set number of estimator for the RandomForestClassifier(1-10). For each iteration I trained the model on the training set and assessed its accuracy on the validation set. The best performing model in terms of accuracy on the validation set had 10 estimators and achieving an accuracy score of approximately 0.821 (82.1%) which is above the 0.75 target threshold for accuracy. I also tested this model with a wider range of n_estimators as well as addinging additonal hyperparameters; however, each time the accuracy decreased and would never get back up to the original 82.1% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy of the logistic regression model on the training set:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7016725009723843"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy of the logistic regression model on the validation set:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7052238805970149"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistics Regression\n",
    "logistics_model = LogisticRegression(random_state=54321, solver='liblinear')\n",
    "logistics_model.fit(features_train, target_train)\n",
    "score_train = logistics_model.score(\n",
    "    features_train,\n",
    "    target_train\n",
    ")  \n",
    "score_valid = logistics_model.score(\n",
    "    features_valid,\n",
    "    target_valid\n",
    ")  \n",
    "\n",
    "display(\n",
    "    \"Accuracy of the logistic regression model on the training set:\",\n",
    "    score_train,\n",
    ")\n",
    "display(\n",
    "    \"Accuracy of the logistic regression model on the validation set:\",\n",
    "    score_valid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the models it seem the Logistics Regression model is the least accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.7807153965785381\n"
     ]
    }
   ],
   "source": [
    "# Check quality using test set\n",
    "best_score = 0\n",
    "best_est = 10 # from previous test\n",
    "random_forest_model = RandomForestClassifier(random_state=54321, n_estimators=est)\n",
    "random_forest_model.fit(features_train, target_train) \n",
    "predictions_test = random_forest_model.predict(test_features)\n",
    "accuracy_test = accuracy_score(test_target, predictions_test)\n",
    "print(\"Accuracy on the test set: {}\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the test set being run we can see that their is a slight disparity in the accuracy of the test set vs the validation set. its not by much and the accuracy score is still over the 0.75 accuracy threshold. However significant differences in accuracy scores of the test set vs the validation set can mean their is overfitting involved or even insufficient model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Baseline accuracy: 0.6951788491446346'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model's accuracy: 0.7807153965785381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sanity check passed: True'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import dummy classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Run dummy classifier model\n",
    "dummy_class_model = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "dummy_class_model.fit(features_train, target_train)\n",
    "dummy_class_predictions = dummy_class_model.predict(test_features)\n",
    "\n",
    "#Baseline model\n",
    "dummy_accuracy = accuracy_score(test_target, dummy_class_predictions)\n",
    "display(f\"Baseline accuracy: {dummy_accuracy}\")\n",
    "\n",
    "# Compare with your model's accuracy\n",
    "model_accuracy = accuracy_score(test_target, predictions_test)\n",
    "print(f\"Your model's accuracy: {model_accuracy}\")\n",
    "\n",
    "# Sanity Check Pass\n",
    "pass_sanity_check = model_accuracy > dummy_accuracy\n",
    "display(f\"Sanity check passed: {pass_sanity_check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sanity check my model accuracy is showing an initial level of quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this testing I have determined that the best model that should be used is the RandomForestClassifier as it has the highers accuracy score. Based on the test set using this model I have been able to determine that 78& of the time the model will correctly identify whether a particular subcriber fits better with the smark plan or the ultra plan, according to the data given."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
